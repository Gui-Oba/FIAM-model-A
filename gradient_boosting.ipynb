{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "939f8be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 245 partitions spanning years 2005-2025\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.dataset as ds\n",
    "import xgboost as xgb\n",
    "\n",
    "# configuration\n",
    "DEVICE = \"cpu\"  # change to \"cuda\" if you have a GPU available\n",
    "ROOT = Path(\"processed_data2\")\n",
    "TARGET = \"stock_ret\"\n",
    "ID_COLS = [\"gvkey\", \"iid\", \"excntry\"]\n",
    "PARTITION_COLS = [\"year\", \"month\"]\n",
    "AUX_DROP = {\"date\", \"char_date\", \"char_eom\", \"ret_eom\", \"prc\", \"id\"}\n",
    "NON_FEATURES = set(ID_COLS + PARTITION_COLS) | AUX_DROP | {TARGET}\n",
    "\n",
    "TRAIN_START = 2005\n",
    "OOS_START, OOS_END = 2015, 2024\n",
    "LOG_DIR = Path(\"train_logs\")\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# tuning config\n",
    "random.seed(0)\n",
    "TUNE_YEAR = 2018\n",
    "N_TRIALS = 20\n",
    "WARM_ROUNDS = 200\n",
    "EARLY_STOP = 30\n",
    "PRUNE_MARGIN = 0.01\n",
    "\n",
    "\n",
    "def list_partitions(root: Path):\n",
    "    months = []\n",
    "    for year_dir in sorted(root.glob(\"year=*\")):\n",
    "        y = int(year_dir.name.split(\"=\")[1])\n",
    "        for month_dir in sorted(year_dir.glob(\"month=*\")):\n",
    "            m = int(month_dir.name.split(\"=\")[1])\n",
    "            months.append((y, m))\n",
    "    months.sort()\n",
    "    return months\n",
    "\n",
    "\n",
    "ALL_MONTHS = list_partitions(ROOT)\n",
    "if not ALL_MONTHS:\n",
    "    raise RuntimeError(f\"No parquet partitions found under {ROOT}\")\n",
    "\n",
    "MONTH_LOOKUP = defaultdict(list)\n",
    "for y, m in ALL_MONTHS:\n",
    "    MONTH_LOOKUP[y].append(m)\n",
    "for months in MONTH_LOOKUP.values():\n",
    "    months.sort()\n",
    "\n",
    "print(f\"loaded {len(ALL_MONTHS)} partitions spanning years {min(MONTH_LOOKUP)}-{max(MONTH_LOOKUP)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a43e147a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retained 39 features; first 10: ['niq_be', 'prc_highprc_252d', 'niq_at', 'mispricing_perf', 'ni_be', 'ebit_bev', 'me', 'ret_12_1', 'ni_me', 'corr_1260d']\n"
     ]
    }
   ],
   "source": [
    "# build feature list from column_names.txt and available parquet columns\n",
    "with open(\"column_names.txt\", \"r\") as f:\n",
    "    requested = [c.strip() for c in f if c.strip()]\n",
    "\n",
    "sample_year, sample_month = ALL_MONTHS[0]\n",
    "sample_path = ROOT / f\"year={sample_year}\" / f\"month={sample_month}\" / \"part-0.parquet\"\n",
    "sample_df = pd.read_parquet(sample_path)\n",
    "\n",
    "available = set(sample_df.columns)\n",
    "FEATURES = [c for c in requested if c in available and c not in NON_FEATURES]\n",
    "if not FEATURES:\n",
    "    raise RuntimeError(\"Feature list is empty after filtering; check column_names.txt and NON_FEATURES settings.\")\n",
    "\n",
    "missing = [c for c in requested if c not in available and c not in NON_FEATURES]\n",
    "if missing:\n",
    "    preview = \", \".join(missing[:10])\n",
    "    print(f\"warning: {len(missing)} requested columns not found in parquet sample (showing up to 10): {preview}\")\n",
    "\n",
    "with open(\"non-features.txt\", \"w\") as out:\n",
    "    for col in sorted((available - set(FEATURES)) - {TARGET}):\n",
    "        out.write(col + \"\\n\")\n",
    "\n",
    "print(f\"retained {len(FEATURES)} features; first 10: {FEATURES[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aeb525d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset():\n",
    "    return ds.dataset(str(ROOT), format=\"parquet\", partitioning=\"hive\")\n",
    "\n",
    "\n",
    "def months_in(year: int):\n",
    "    return MONTH_LOOKUP.get(year, [])\n",
    "\n",
    "\n",
    "class ArrowMonthIter(xgb.core.DataIter):\n",
    "    \"\"\"Stream month-sliced panels into XGBoost without loading the full dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, y_start, y_end, features, target, batch_size=131072):\n",
    "        super().__init__()\n",
    "        self.y_start = y_start\n",
    "        self.y_end = y_end\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.batch_size = batch_size\n",
    "        self._plan = [(y, m) for y in range(y_start, y_end + 1) for m in months_in(y)]\n",
    "        self._dataset = None\n",
    "        self._scanner_iter = None\n",
    "        self._idx = 0\n",
    "\n",
    "    def reset(self):\n",
    "        if not self._plan:\n",
    "            raise ValueError(f\"No partitions available between years {self.y_start} and {self.y_end}.\")\n",
    "        self._dataset = dataset()\n",
    "        self._idx = 0\n",
    "        self._scanner_iter = None\n",
    "\n",
    "    def next(self, input_data):\n",
    "        while True:\n",
    "            if self._scanner_iter is None:\n",
    "                if self._idx >= len(self._plan):\n",
    "                    return 0\n",
    "                y, m = self._plan[self._idx]\n",
    "                self._idx += 1\n",
    "                filt = (ds.field(\"year\") == y) & (ds.field(\"month\") == m)\n",
    "                cols = self.features + [self.target]\n",
    "                scanner = self._dataset.scanner(columns=cols, filter=filt, batch_size=self.batch_size)\n",
    "                self._scanner_iter = iter(scanner.to_batches())\n",
    "            try:\n",
    "                batch = next(self._scanner_iter)\n",
    "            except StopIteration:\n",
    "                self._scanner_iter = None\n",
    "                continue\n",
    "            X = np.column_stack([batch.column(name).to_numpy(zero_copy_only=False) for name in self.features]).astype(np.float32, copy=False)\n",
    "            y = batch.column(self.target).to_numpy(zero_copy_only=False).astype(np.float32, copy=False)\n",
    "            if X.size == 0:\n",
    "                continue\n",
    "            input_data(data=X, label=y)\n",
    "            return 1\n",
    "\n",
    "\n",
    "def _build_dmats(train_end, val_start, val_end, max_bin):\n",
    "    it_tr = ArrowMonthIter(TRAIN_START, train_end, FEATURES, TARGET)\n",
    "    it_va = ArrowMonthIter(val_start, val_end, FEATURES, TARGET)\n",
    "    dtr = xgb.QuantileDMatrix(it_tr, missing=np.nan, max_bin=max_bin)\n",
    "    dva = xgb.QuantileDMatrix(it_va, ref=dtr, missing=np.nan, max_bin=max_bin)\n",
    "    return dtr, dva\n",
    "\n",
    "\n",
    "def _train_until(dtr, dva, params, max_rounds):\n",
    "    booster = xgb.train(\n",
    "        params,\n",
    "        dtr,\n",
    "        num_boost_round=max_rounds,\n",
    "        evals=[(dva, \"val\")],\n",
    "        early_stopping_rounds=EARLY_STOP,\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "    rmse = float(booster.best_score)\n",
    "    best_it = booster.best_iteration if booster.best_iteration is not None else max_rounds - 1\n",
    "    preds = booster.predict(dva, iteration_range=(0, best_it + 1))\n",
    "    sigma_pred = float(np.std(preds))\n",
    "    return booster, rmse, best_it, sigma_pred\n",
    "\n",
    "\n",
    "def tune_once():\n",
    "    if not months_in(TUNE_YEAR):\n",
    "        raise ValueError(f\"No data available for TUNE_YEAR={TUNE_YEAR}. Adjust the constant or regenerate processed_data2.\")\n",
    "\n",
    "    base = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"device\": DEVICE,\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"lambda\": 5.0,\n",
    "        \"alpha\": 0.0,\n",
    "        \"seed\": 0,\n",
    "        \"nthread\": -1,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "    }\n",
    "\n",
    "    mb_candidates = [256, 512]\n",
    "    mcw_candidates = [1, 20]\n",
    "    depths = [4, 6]\n",
    "    etas = [0.03, 0.05]\n",
    "    subs = [0.7, 0.9]\n",
    "\n",
    "    trials = [\n",
    "        {\n",
    "            \"max_depth\": random.choice(depths),\n",
    "            \"eta\": random.choice(etas),\n",
    "            \"subsample\": random.choice(subs),\n",
    "            \"min_child_weight\": random.choice(mcw_candidates),\n",
    "            \"max_bin\": random.choice(mb_candidates),\n",
    "        }\n",
    "        for _ in range(N_TRIALS)\n",
    "    ]\n",
    "\n",
    "    dmat_cache = {}\n",
    "\n",
    "    def get_dmats(max_bin):\n",
    "        if max_bin not in dmat_cache:\n",
    "            dmat_cache[max_bin] = _build_dmats(TUNE_YEAR - 3, TUNE_YEAR - 2, TUNE_YEAR - 1, max_bin)\n",
    "        return dmat_cache[max_bin]\n",
    "\n",
    "    best_rmse = float(\"inf\")\n",
    "    best_payload = None\n",
    "\n",
    "    for trial in trials:\n",
    "        params = dict(base, **trial)\n",
    "        dtr, dva = get_dmats(trial[\"max_bin\"])\n",
    "        _, warm_rmse, _, _ = _train_until(dtr, dva, params, WARM_ROUNDS)\n",
    "        if best_payload is not None and warm_rmse > best_rmse * (1.0 + PRUNE_MARGIN):\n",
    "            continue\n",
    "        booster, rmse, best_it, sigma_pred = _train_until(dtr, dva, params, 2000)\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_payload = {\n",
    "                \"params\": params,\n",
    "                \"best_iteration\": best_it,\n",
    "                \"sigma_pred_val\": sigma_pred,\n",
    "            }\n",
    "\n",
    "    if best_payload is None:\n",
    "        best_payload = {\n",
    "            \"params\": dict(base, max_depth=6, eta=0.05, subsample=0.9, min_child_weight=20, max_bin=256),\n",
    "            \"best_iteration\": 1000,\n",
    "            \"sigma_pred_val\": None,\n",
    "        }\n",
    "\n",
    "    (LOG_DIR / \"global_best.json\").write_text(json.dumps(best_payload, indent=2))\n",
    "    print(\"tuned_params:\", json.dumps(best_payload, indent=2))\n",
    "    return best_payload\n",
    "\n",
    "\n",
    "def get_best_params():\n",
    "    payload_path = LOG_DIR / \"global_best.json\"\n",
    "    if payload_path.exists():\n",
    "        return json.loads(payload_path.read_text())\n",
    "    return tune_once()\n",
    "\n",
    "\n",
    "def fit_for_oos_year(oos_year: int):\n",
    "    if not months_in(oos_year - 1):\n",
    "        raise ValueError(f\"Insufficient history to fit year {oos_year}. Ensure previous months exist in processed_data2.\")\n",
    "    best = get_best_params()\n",
    "    max_bin = best[\"params\"][\"max_bin\"]\n",
    "    dtr, dva = _build_dmats(oos_year - 3, oos_year - 2, oos_year - 1, max_bin)\n",
    "    booster = xgb.train(\n",
    "        best[\"params\"],\n",
    "        dtr,\n",
    "        num_boost_round=5000,\n",
    "        evals=[(dva, \"val\")],\n",
    "        early_stopping_rounds=EARLY_STOP,\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "    return booster\n",
    "\n",
    "\n",
    "def predict_year(booster: xgb.Booster, year: int, out_root: Path):\n",
    "    months = months_in(year)\n",
    "    if not months:\n",
    "        print(f\"skip {year}: no partitions to score\")\n",
    "        return\n",
    "\n",
    "    out_root.mkdir(exist_ok=True)\n",
    "    dset = dataset()\n",
    "\n",
    "    for month in months:\n",
    "        filt = (ds.field(\"year\") == year) & (ds.field(\"month\") == month)\n",
    "        cols = FEATURES + ID_COLS + PARTITION_COLS\n",
    "        batches = dset.scanner(columns=cols, filter=filt, batch_size=131072).to_batches()\n",
    "        if not batches:\n",
    "            continue\n",
    "\n",
    "        rows = []\n",
    "        preds = []\n",
    "        for batch in batches:\n",
    "            X = np.column_stack([batch.column(name).to_numpy(zero_copy_only=False) for name in FEATURES]).astype(np.float32, copy=False)\n",
    "            dm = xgb.DMatrix(X, missing=np.nan)\n",
    "            pred = booster.predict(dm)\n",
    "            ids = {col: batch.column(col).to_numpy(zero_copy_only=False) for col in ID_COLS + PARTITION_COLS}\n",
    "            rows.append(ids)\n",
    "            preds.append(pred.astype(np.float32, copy=False))\n",
    "\n",
    "        if not rows:\n",
    "            continue\n",
    "\n",
    "        merged = {col: np.concatenate([chunk[col] for chunk in rows]) for col in rows[0]}\n",
    "        out_df = pd.DataFrame(merged)\n",
    "        out_df[\"pred_ret_t1\"] = np.concatenate(preds)\n",
    "        out_df = out_df.groupby(ID_COLS + PARTITION_COLS, as_index=False)[\"pred_ret_t1\"].mean()\n",
    "        sigma = float(out_df[\"pred_ret_t1\"].std())\n",
    "        print(f\"year {year} month {month}: sigma_pred {sigma:.6f}\")\n",
    "\n",
    "        dest = out_root / f\"year={year}\" / f\"month={month}\"\n",
    "        dest.mkdir(parents=True, exist_ok=True)\n",
    "        out_df.to_parquet(dest / \"part-0.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3edd8bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored on calling ctypes callback function <bound method DataIter._next_wrapper of <__main__.ArrowMonthIter object at 0x12ff2fb10>>:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gui.oba/Documents/Coding/FIAM/.venv/lib/python3.13/site-packages/xgboost/core.py\", line 640, in _next_wrapper\n",
      "    return self._handle_exception(lambda: int(self.next(input_data)), 0)\n",
      "  File \"/Users/gui.oba/Documents/Coding/FIAM/.venv/lib/python3.13/site-packages/xgboost/core.py\", line 553, in _handle_exception\n",
      "    return fn()\n",
      "  File \"/Users/gui.oba/Documents/Coding/FIAM/.venv/lib/python3.13/site-packages/xgboost/core.py\", line 640, in <lambda>\n",
      "    return self._handle_exception(lambda: int(self.next(input_data)), 0)\n",
      "  File \"/var/folders/23/bz30dzh9653db9qn7hz3ydqm0000gn/T/ipykernel_58234/3266367322.py\", line 43, in next\n",
      "  File \"pyarrow/_dataset.pyx\", line 3904, in _iterator\n",
      "  File \"pyarrow/_dataset.pyx\", line 3497, in pyarrow._dataset.TaggedRecordBatchIterator.__next__\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[17:12:16] /Users/runner/work/xgboost/xgboost/src/data/iterative_dmatrix.cc:99: Check failed: rbegin == Info().num_row_ (615992 vs. 616065) : \nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x000000012e1d2254 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x000000012e382798 xgboost::data::IterativeDMatrix::InitFromCPU(xgboost::Context const*, xgboost::BatchParam const&, void*, float, std::__1::shared_ptr<xgboost::DMatrix>) + 2796\n  [bt] (2) 3   libxgboost.dylib                    0x000000012e381884 xgboost::data::IterativeDMatrix::IterativeDMatrix(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int, long long) + 788\n  [bt] (3) 4   libxgboost.dylib                    0x000000012e324f90 xgboost::DMatrix* xgboost::DMatrix::Create<void*, void*, void (void*), int (void*)>(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int, long long) + 152\n  [bt] (4) 5   libxgboost.dylib                    0x000000012e1dc254 XGQuantileDMatrixCreateFromCallback + 520\n  [bt] (5) 6   libffi.dylib                        0x00000001a2ef8050 ffi_call_SYSV + 80\n  [bt] (6) 7   libffi.dylib                        0x00000001a2f00ae0 ffi_call_int + 1212\n  [bt] (7) 8   _ctypes.cpython-313-darwin.so       0x000000011190f838 _ctypes_callproc + 940\n  [bt] (8) 9   _ctypes.cpython-313-darwin.so       0x0000000111905480 PyCFuncPtr_call + 256\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXGBoostError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# run tuning once (writes train_logs/global_best.json)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# best_params = tune_once()\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m best_params = \u001b[43mget_best_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(json.dumps(best_params, indent=\u001b[32m2\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 153\u001b[39m, in \u001b[36mget_best_params\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m payload_path.exists():\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m json.loads(payload_path.read_text())\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtune_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 124\u001b[39m, in \u001b[36mtune_once\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m trial \u001b[38;5;129;01min\u001b[39;00m trials:\n\u001b[32m    123\u001b[39m     params = \u001b[38;5;28mdict\u001b[39m(base, **trial)\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     dtr, dva = \u001b[43mget_dmats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_bin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m     _, warm_rmse, _, _ = _train_until(dtr, dva, params, WARM_ROUNDS)\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m best_payload \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m warm_rmse > best_rmse * (\u001b[32m1.0\u001b[39m + PRUNE_MARGIN):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 116\u001b[39m, in \u001b[36mtune_once.<locals>.get_dmats\u001b[39m\u001b[34m(max_bin)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_dmats\u001b[39m(max_bin):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m max_bin \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dmat_cache:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         dmat_cache[max_bin] = \u001b[43m_build_dmats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTUNE_YEAR\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTUNE_YEAR\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTUNE_YEAR\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dmat_cache[max_bin]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36m_build_dmats\u001b[39m\u001b[34m(train_end, val_start, val_end, max_bin)\u001b[39m\n\u001b[32m     57\u001b[39m it_va = ArrowMonthIter(val_start, val_end, FEATURES, TARGET)\n\u001b[32m     58\u001b[39m dtr = xgb.QuantileDMatrix(it_tr, missing=np.nan, max_bin=max_bin)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m dva = \u001b[43mxgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mQuantileDMatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit_va\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_bin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dtr, dva\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Coding/FIAM/.venv/lib/python3.13/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Coding/FIAM/.venv/lib/python3.13/site-packages/xgboost/core.py:1614\u001b[39m, in \u001b[36mQuantileDMatrix.__init__\u001b[39m\u001b[34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, max_quantile_batches, data_split_mode)\u001b[39m\n\u001b[32m   1594\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m   1595\u001b[39m         info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   (...)\u001b[39m\u001b[32m   1607\u001b[39m         )\n\u001b[32m   1608\u001b[39m     ):\n\u001b[32m   1609\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1610\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mIf data iterator is used as input, data like label should be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1611\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mspecified as batch argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1612\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1614\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1615\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1616\u001b[39m \u001b[43m    \u001b[49m\u001b[43mref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1617\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1618\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1619\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1620\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1627\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_quantile_blocks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_quantile_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Coding/FIAM/.venv/lib/python3.13/site-packages/xgboost/core.py:1680\u001b[39m, in \u001b[36mQuantileDMatrix._init\u001b[39m\u001b[34m(self, data, ref, enable_categorical, max_quantile_blocks, **meta)\u001b[39m\n\u001b[32m   1678\u001b[39m it.reraise()\n\u001b[32m   1679\u001b[39m \u001b[38;5;66;03m# delay check_call to throw intermediate exception first\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1680\u001b[39m \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1681\u001b[39m \u001b[38;5;28mself\u001b[39m.handle = handle\n\u001b[32m   1683\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ref \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Coding/FIAM/.venv/lib/python3.13/site-packages/xgboost/core.py:310\u001b[39m, in \u001b[36m_check_call\u001b[39m\u001b[34m(ret)\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[32m    300\u001b[39m \n\u001b[32m    301\u001b[39m \u001b[33;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m \u001b[33;03m    return value from API calls\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret != \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "\u001b[31mXGBoostError\u001b[39m: [17:12:16] /Users/runner/work/xgboost/xgboost/src/data/iterative_dmatrix.cc:99: Check failed: rbegin == Info().num_row_ (615992 vs. 616065) : \nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x000000012e1d2254 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x000000012e382798 xgboost::data::IterativeDMatrix::InitFromCPU(xgboost::Context const*, xgboost::BatchParam const&, void*, float, std::__1::shared_ptr<xgboost::DMatrix>) + 2796\n  [bt] (2) 3   libxgboost.dylib                    0x000000012e381884 xgboost::data::IterativeDMatrix::IterativeDMatrix(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int, long long) + 788\n  [bt] (3) 4   libxgboost.dylib                    0x000000012e324f90 xgboost::DMatrix* xgboost::DMatrix::Create<void*, void*, void (void*), int (void*)>(void*, void*, std::__1::shared_ptr<xgboost::DMatrix>, void (*)(void*), int (*)(void*), float, int, int, long long) + 152\n  [bt] (4) 5   libxgboost.dylib                    0x000000012e1dc254 XGQuantileDMatrixCreateFromCallback + 520\n  [bt] (5) 6   libffi.dylib                        0x00000001a2ef8050 ffi_call_SYSV + 80\n  [bt] (6) 7   libffi.dylib                        0x00000001a2f00ae0 ffi_call_int + 1212\n  [bt] (7) 8   _ctypes.cpython-313-darwin.so       0x000000011190f838 _ctypes_callproc + 940\n  [bt] (8) 9   _ctypes.cpython-313-darwin.so       0x0000000111905480 PyCFuncPtr_call + 256\n\n"
     ]
    }
   ],
   "source": [
    "# run tuning once (writes train_logs/global_best.json)\n",
    "# best_params = tune_once()\n",
    "best_params = get_best_params()\n",
    "print(json.dumps(best_params, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e02a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(OOS_START, OOS_END + 1):\n",
    "    if not months_in(year):\n",
    "        print(f\"skip {year}: no data in processed_data2\")\n",
    "        continue\n",
    "    bst = fit_for_oos_year(year)\n",
    "    predict_year(bst, year, Path(\"oos_preds\"))\n",
    "    print(f\"year {year}: prediction pass done\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
